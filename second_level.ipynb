{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cab43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from nilearn import plotting, image\n",
    "from nilearn.glm.second_level import SecondLevelModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b12a0",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "Define paths and subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7f10d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects: 21\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "first_level_dir = \"first_level_results\"\n",
    "output_dir = \"second_level_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List of subjects\n",
    "subjects = [f\"sub-{i:02d}\" for i in range(1, 22)]\n",
    "print(f\"Subjects: {len(subjects)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d954a70",
   "metadata": {},
   "source": [
    "## 2. Collect First-Level Maps\n",
    "We collect the Z-maps for the contrast `incongruent_vs_congruent` generated in the first-level analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee498ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 maps for second-level analysis.\n"
     ]
    }
   ],
   "source": [
    "contrast_id = \"incongruent_vs_congruent\"\n",
    "input_maps = []\n",
    "\n",
    "for sub in subjects:\n",
    "    map_path = os.path.join(first_level_dir, f\"{sub}_zmap_{contrast_id}.nii.gz\")\n",
    "    if os.path.exists(map_path):\n",
    "        input_maps.append(map_path)\n",
    "    else:\n",
    "        print(f\"Warning: Map not found for {sub} at {map_path}\")\n",
    "\n",
    "print(f\"Found {len(input_maps)} maps for second-level analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa09c8dd",
   "metadata": {},
   "source": [
    "## 3. Second-Level Model Fitting\n",
    "Fit a One-Sample T-Test to determine group-level activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfab2825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Second-Level Model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Field of view of image #1 is different from reference FOV.\nReference affine:\narray([[   3.        ,    0.        ,    0.        ,  -96.66867828],\n       [   0.        ,    3.        ,    0.        , -106.78915405],\n       [   0.        ,    0.        ,    4.        ,  -81.61445618],\n       [   0.        ,    0.        ,    0.        ,    1.        ]])\nImage affine:\narray([[  3.        ,   0.        ,   0.        , -80.04216766],\n       [  0.        ,   3.        ,   0.        , -91.60843658],\n       [  0.        ,   0.        ,   4.        , -80.89156342],\n       [  0.        ,   0.        ,   0.        ,   1.        ]])\nReference shape:\n(64, 64, 40)\nImage shape:\n(64, 64, 40, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m group_model.fit(input_maps, design_matrix=design_matrix)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Compute Group Contrast (intercept)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m z_map_group = \u001b[43mgroup_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mz_score\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Save result\u001b[39;00m\n\u001b[32m     23\u001b[39m group_out_path = os.path.join(output_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgroup_zmap_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontrast_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.nii.gz\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/neurocomp/lib/python3.12/site-packages/nilearn/glm/second_level/second_level.py:736\u001b[39m, in \u001b[36mSecondLevelModel.compute_contrast\u001b[39m\u001b[34m(self, second_level_contrast, first_level_contrast, second_level_stat_type, output_type)\u001b[39m\n\u001b[32m    733\u001b[39m _check_n_rows_desmat_vs_n_effect_maps(effect_maps, \u001b[38;5;28mself\u001b[39m.design_matrix_)\n\u001b[32m    735\u001b[39m \u001b[38;5;66;03m# Fit an Ordinary Least Squares regression for parametric statistics\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m Y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmasker_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43meffect_maps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.memory_:\n\u001b[32m    738\u001b[39m     mem_glm = \u001b[38;5;28mself\u001b[39m._cache(run_glm, ignore=[\u001b[33m\"\u001b[39m\u001b[33mn_jobs\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/neurocomp/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/neurocomp/lib/python3.12/site-packages/nilearn/maskers/base_masker.py:361\u001b[39m, in \u001b[36mBaseMasker.transform\u001b[39m\u001b[34m(self, imgs, confounds, sample_mask)\u001b[39m\n\u001b[32m    358\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m confounds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.high_variance_confounds:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform_single_imgs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_mask\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# Compute high variance confounds if requested\u001b[39;00m\n\u001b[32m    366\u001b[39m all_confounds = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/neurocomp/lib/python3.12/site-packages/nilearn/maskers/nifti_masker.py:666\u001b[39m, in \u001b[36mNiftiMasker.transform_single_imgs\u001b[39m\u001b[34m(self, imgs, confounds, sample_mask, copy)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clean_kwargs:\n\u001b[32m    664\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mclean_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.clean_kwargs_\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_and_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbose\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_level\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcopy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshelve\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shelving\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask_img_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_level\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmemory_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmemory_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/neurocomp/lib/python3.12/site-packages/joblib/memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/neurocomp/lib/python3.12/site-packages/nilearn/maskers/nifti_masker.py:155\u001b[39m, in \u001b[36mfilter_and_mask\u001b[39m\u001b[34m(imgs, mask_img_, parameters, memory_level, memory, verbose, confounds, sample_mask, copy, dtype)\u001b[39m\n\u001b[32m    151\u001b[39m     memory = Memory(location=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# Convert input to niimg to check shape.\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This must be repeated after the shape check because check_niimg will\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# coerce 5D data to 4D, which we don't want.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m temp_imgs = \u001b[43mcheck_niimg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m imgs = check_niimg(imgs, atleast_4d=\u001b[38;5;28;01mTrue\u001b[39;00m, ensure_ndim=\u001b[32m4\u001b[39m)\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# Check whether resampling is truly necessary. If so, crop mask\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# as small as possible in order to speed up the process\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/neurocomp/lib/python3.12/site-packages/nilearn/_utils/niimg_conversions.py:349\u001b[39m, in \u001b[36mcheck_niimg\u001b[39m\u001b[34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001b[39m\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_iterator:\n\u001b[32m    346\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m iter_check_niimg(\n\u001b[32m    347\u001b[39m             niimg, ensure_ndim=ensure_ndim, dtype=dtype\n\u001b[32m    348\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mni\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat_imgs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mniimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ndim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_ndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# Otherwise, it should be a filename or a SpatialImage, we load it\u001b[39;00m\n\u001b[32m    354\u001b[39m niimg = load_niimg(niimg, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/neurocomp/lib/python3.12/site-packages/nilearn/image/image.py:1838\u001b[39m, in \u001b[36mconcat_imgs\u001b[39m\u001b[34m(niimgs, dtype, ensure_ndim, memory, memory_level, auto_resample, verbose)\u001b[39m\n\u001b[32m   1836\u001b[39m data = np.ndarray((*target_shape, \u001b[38;5;28msum\u001b[39m(lengths)), order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m, dtype=dtype)\n\u001b[32m   1837\u001b[39m cur_4d_index = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1838\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mniimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1839\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1841\u001b[39m \u001b[43m        \u001b[49m\u001b[43miter_check_niimg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1842\u001b[39m \u001b[43m            \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1843\u001b[39m \u001b[43m            \u001b[49m\u001b[43matleast_4d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1844\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtarget_fov\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_fov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmemory_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1847\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnii_str\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mniimg\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mniimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage #\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mindex\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1852\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mConcatenating \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mindex\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m+\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnii_str\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/neurocomp/lib/python3.12/site-packages/nilearn/_utils/niimg_conversions.py:174\u001b[39m, in \u001b[36miter_check_niimg\u001b[39m\u001b[34m(niimgs, ensure_ndim, atleast_4d, target_fov, dtype, memory, memory_level)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _check_fov(niimg, ref_fov[\u001b[32m0\u001b[39m], ref_fov[\u001b[32m1\u001b[39m]):\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m target_fov \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    175\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mField of view of image #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is different from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    176\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreference FOV.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    177\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReference affine:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mref_fov[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage affine:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mniimg.affine\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    179\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReference shape:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mref_fov[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage shape:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mniimg.shape\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m         )\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnilearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m image  \u001b[38;5;66;03m# we avoid a circular import\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resample_to_first_img:\n",
      "\u001b[31mValueError\u001b[39m: Field of view of image #1 is different from reference FOV.\nReference affine:\narray([[   3.        ,    0.        ,    0.        ,  -96.66867828],\n       [   0.        ,    3.        ,    0.        , -106.78915405],\n       [   0.        ,    0.        ,    4.        ,  -81.61445618],\n       [   0.        ,    0.        ,    0.        ,    1.        ]])\nImage affine:\narray([[  3.        ,   0.        ,   0.        , -80.04216766],\n       [  0.        ,   3.        ,   0.        , -91.60843658],\n       [  0.        ,   0.        ,   4.        , -80.89156342],\n       [  0.        ,   0.        ,   0.        ,   1.        ]])\nReference shape:\n(64, 64, 40)\nImage shape:\n(64, 64, 40, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting Second-Level Model...\")\n",
    "\n",
    "# Define model (One-Sample T-Test)\n",
    "group_model = SecondLevelModel(\n",
    "    smoothing_fwhm=8.0,  # Standard smoothing for group level\n",
    "    minimize_memory=True\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "if not input_maps:\n",
    "    raise ValueError(\"No input maps found. Please run the first-level analysis first.\")\n",
    "\n",
    "# Create design matrix (One-Sample T-Test)\n",
    "# Following nilearn example: plot_second_level_one_sample_test.py\n",
    "design_matrix = pd.DataFrame([1] * len(input_maps), columns=['intercept'])\n",
    "\n",
    "group_model.fit(input_maps, design_matrix=design_matrix)\n",
    "\n",
    "# Compute Group Contrast (intercept)\n",
    "z_map_group = group_model.compute_contrast(output_type='z_score')\n",
    "\n",
    "# Save result\n",
    "group_out_path = os.path.join(output_dir, f\"group_zmap_{contrast_id}.nii.gz\")\n",
    "z_map_group.to_filename(group_out_path)\n",
    "print(f\"Group Z-map saved to: {group_out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45277ff0",
   "metadata": {},
   "source": [
    "## 4. Generate Report\n",
    "Create an HTML report for the group analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc594106",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating Group Level Report...\")\n",
    "report = group_model.generate_report(\n",
    "    contrasts=\"intercept\",\n",
    "    title=\"Group Level Result: Incongruent > Congruent\"\n",
    ")\n",
    "\n",
    "report_path = os.path.join(output_dir, \"report_group_level.html\")\n",
    "report.save_as_html(report_path)\n",
    "print(f\"Group report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bae81e",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "Plot the group-level statistical map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665690b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(\n",
    "    z_map_group, \n",
    "    title=\"Group Level: Incongruent > Congruent (Z > 3.0)\", \n",
    "    threshold=3.0\n",
    ")\n",
    "plotting.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurocomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
